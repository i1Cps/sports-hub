<div align="center">
  <h1>Sprint Quadruped</h1>
</div>

<img width="1818" height="730" alt="quadruped_sprint" src="https://github.com/user-attachments/assets/fd9a904c-7bb6-47ca-b4ae-ce2dfd81cf9d" />

<br/>

<p align="center">
  <a href="https://i1cps.github.io/Sports-Hub-Website/"><strong>ğŸ”— Click for live demo </strong></a>
</p>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png)

## ğŸ§© Difficulty

<p align="center" style="font-size:1.1em">
â­â˜†â˜†â˜†â˜†â˜†â˜†â˜†â˜†â˜†<br/>
<em>1/10: Simple joint coordination in a straight line</em>
</p>

This is one of the simplest environments. The Quadruped just needs to learn how to move its joints rhythmically to build momentum and sprint in a straight line, no turning, balancing objects, or reacting to obstacles.

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png)

## ğŸ§® Reward Function


Each timestep the agent receives a reward composed of three terms:

- ğŸƒâ€â™‚ï¸ **Forward Velocity**  
  Encourages the humanoid to sprint forward along the xâ€‘axis.

- â¤ï¸ **Alive Bonus**  
  A constant bonus awarded per timestep.
  
- âš™ï¸ **Control Cost**  
  Penalizes large actuator torques to promote efficient, smooth motions.

  ![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png)

## ğŸ” Reset Logic

Each of the **8192 parallel environments** are initialized with small uniform noise [-0.1, +0.1] applied to joint positions and velocities of the quadruped. This encourages robustness and prevents agents from overfitting to a single deterministic start state.

The episode resets early if the quadruped's **torso tilts beyond an angle threshold** relative to the z-axis (vertical axis)   
i.e. its about to flip over. This drastically speeds up learning.

```python
  def _compute_termination(self, data: mjx.Data) -> jax.Array:
      # Get the angle of the quadrupeds torso, and make sure its within the angle limit
      torso_angle = jp.ravel(data.site_xmat[self._agent_site_index])[8]
      upright = torso_angle > self._agent_healthy_torso_angle

      reset_condition = jp.logical_not(upright)
      return jp.where(reset_condition, 1.0, 0.0)
```

> In evaluation mode, the environment also resets when the agent **crosses the finish line**, simulating a race format.

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png)

## ğŸ‘ Observation Function

The agent receives a **27â€‘dimensional** observation vector containing only locomotion-relevant information:

- **13â€¯ Joint Positions**   
  Local joint angles, root height and orientation (quat)

- **14â€¯ Joint Velocities**  
  Angular and linear velocity terms for each joint.

> ğŸ—’ï¸ **Total:** 13 + 14 = **27** dimensions

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png)

## ğŸ—ï¸ MJCF Highlights

âš ï¸ Notable design choices:

The model of the quadruped used here is from the brax package meaning it has the following changes:
  - `density="5.0"`

This means the quadruped is modelled as weighing similar to foam/sponge, far lighter than materials like plastic ~1000â€¯kg/mÂ³.

Other notable design choices in the `model.xml` that support fast end-to-end JAX training:
- **â±ï¸ Timestep = 0.004**  
  A small simulation step allows for smoother dynamics and better contact stability without overwhelming compute.

- **âš¡ Lightweight contact modeling**  
  Only a minimal set of contact pairs are defined (e.g., feet-to-track, some self-collisions), reducing simulation overhead and accelerating JAX-based rollouts.

> ğŸ’¡ These modeling choices keep the simulation light and fast, ideal for scaling to thousands of environments in parallel.

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png)


## ğŸ“Š Hyperparameters

The following settings (from `hyperparams.json`) were used to train the model with PPO:

| Parameter               | Value       |
| ----------------------- | ----------- |
| Total timesteps         | 53,000,000  |
| Learning rate           | 3â€¯Ã—â€¯10â»â´    |
| Num environments        | 8,192       |
| Unroll length           | 10 steps    |
| Batch size              | 256         |
| Episode length          | 1,000 steps |
| Num minibatches         | 32          |
| Updates per batch       | 5           |
| Discount factor (Î³)     | 0.99        |
| Entropy cost            | 0.01        |
| Max gradient norm       | 1.0         |
| PPO clipping Îµ          | 0.2         |
| Reward scaling          | 0.1         |
| Action repeat           | 1           |
| Normalize observations  | True        |
| RNG seed                | 14          |
| Num evaluation rollouts | 10          |

These are tuned for fast, stable PPO convergence

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png)

## ğŸ§ª Experiment with quadruped model

Try modelling the quadruped with tendons instead, similar to the [quadruped in DM_Control](https://github.com/google-deepmind/dm_control/blob/ad0e00871ae3b1801faa3c64c9adc85d818ac06d/dm_control/suite/quadruped.xml#L208):

```xml
<tendon>
  <fixed name="extend_front_left">
    <joint joint="quadruped_front_left_hip"   coef=".25"/>
    <joint joint="quadruped_front_left_ankle" coef=".25"/>
  </fixed>
  <fixed name="lift_front_left">
    <joint joint="quadruped_front_left_hip"   coef=".5"/>
    <joint joint="quadruped_front_left_ankle" coef="-.5"/>
  </fixed>
  ...
</tendon>

<actuator>
  <general name="lift_front_left"   tendon="lift_front_left"   ctrlrange="-1.0 1.0"/>
  <general name="extend_front_left" tendon="extend_front_left" ctrlrange="-1.0 1.0"/>
  ...
</actuator>
```

Or invent a race mode where finishing fast **terminates early and gives a bonus**.



![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png)

Ready to race? This environment is a solid benchmark for dynamic locomotion and full-body coordination.



